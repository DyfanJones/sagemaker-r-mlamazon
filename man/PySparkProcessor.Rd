% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/spark_processing.R
\name{PySparkProcessor}
\alias{PySparkProcessor}
\title{PySparkProcessor Class}
\description{
Handles Amazon SageMaker processing tasks for jobs using PySpark.
}
\section{Super classes}{
\code{\link[sagemaker.common:Processor]{sagemaker.common::Processor}} -> \code{\link[sagemaker.common:ScriptProcessor]{sagemaker.common::ScriptProcessor}} -> \code{\link[sagemaker.mlframework:.SparkProcessorBase]{sagemaker.mlframework::.SparkProcessorBase}} -> \code{PySparkProcessor}
}
\section{Methods}{
\subsection{Public methods}{
\itemize{
\item \href{#method-new}{\code{PySparkProcessor$new()}}
\item \href{#method-get_args_run}{\code{PySparkProcessor$get_args_run()}}
\item \href{#method-run}{\code{PySparkProcessor$run()}}
\item \href{#method-clone}{\code{PySparkProcessor$clone()}}
}
}
\if{html}{
\out{<details open ><summary>Inherited methods</summary>}
\itemize{
\item \out{<span class="pkg-link" data-pkg="sagemaker.common" data-topic="Processor" data-id="format">}\href{../../sagemaker.common/html/Processor.html#method-format}{\code{sagemaker.common::Processor$format()}}\out{</span>}
\item \out{<span class="pkg-link" data-pkg="sagemaker.mlframework" data-topic=".SparkProcessorBase" data-id="get_run_args">}\href{../../sagemaker.mlframework/html/.SparkProcessorBase.html#method-get_run_args}{\code{sagemaker.mlframework::.SparkProcessorBase$get_run_args()}}\out{</span>}
\item \out{<span class="pkg-link" data-pkg="sagemaker.mlframework" data-topic=".SparkProcessorBase" data-id="start_history">}\href{../../sagemaker.mlframework/html/.SparkProcessorBase.html#method-start_history}{\code{sagemaker.mlframework::.SparkProcessorBase$start_history()}}\out{</span>}
\item \out{<span class="pkg-link" data-pkg="sagemaker.mlframework" data-topic=".SparkProcessorBase" data-id="terminate_history_server">}\href{../../sagemaker.mlframework/html/.SparkProcessorBase.html#method-terminate_history_server}{\code{sagemaker.mlframework::.SparkProcessorBase$terminate_history_server()}}\out{</span>}
}
\out{</details>}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-new"></a>}}
\if{latex}{\out{\hypertarget{method-new}{}}}
\subsection{Method \code{new()}}{
Initialize an ``PySparkProcessor`` instance.
             The PySparkProcessor handles Amazon SageMaker processing tasks for jobs
             using SageMaker PySpark.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{PySparkProcessor$new(
  role,
  instance_type,
  instance_count,
  framework_version = NULL,
  py_version = NULL,
  container_version = NULL,
  image_uri = NULL,
  volume_size_in_gb = 30,
  volume_kms_key = NULL,
  output_kms_key = NULL,
  max_runtime_in_seconds = NULL,
  base_job_name = NULL,
  sagemaker_session = NULL,
  env = NULL,
  tags = NULL,
  network_config = NULL
)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{role}}{(str): An AWS IAM role name or ARN. The Amazon SageMaker training jobs
and APIs that create Amazon SageMaker endpoints use this role
to access training data and model artifacts. After the endpoint
is created, the inference code might use the IAM role, if it
needs to access an AWS resource.}

\item{\code{instance_type}}{(str): Type of EC2 instance to use for
processing, for example, 'ml.c4.xlarge'.}

\item{\code{instance_count}}{(int): The number of instances to run
the Processing job with. Defaults to 1.}

\item{\code{framework_version}}{(str): The version of SageMaker PySpark.}

\item{\code{py_version}}{(str): The version of python.}

\item{\code{container_version}}{(str): The version of spark container.}

\item{\code{image_uri}}{(str): The container image to use for training.}

\item{\code{volume_size_in_gb}}{(int): Size in GB of the EBS volume to
use for storing data during processing (default: 30).}

\item{\code{volume_kms_key}}{(str): A KMS key for the processing
volume.}

\item{\code{output_kms_key}}{(str): The KMS key id for all ProcessingOutputs.}

\item{\code{max_runtime_in_seconds}}{(int): Timeout in seconds.
After this amount of time Amazon SageMaker terminates the job
regardless of its current status.}

\item{\code{base_job_name}}{(str): Prefix for processing name. If not specified,
the processor generates a default job name, based on the
training image name and current timestamp.}

\item{\code{sagemaker_session}}{(sagemaker.session.Session): Session object which
manages interactions with Amazon SageMaker APIs and any other
AWS services needed. If not specified, the processor creates one
using the default AWS configuration chain.}

\item{\code{env}}{(dict): Environment variables to be passed to the processing job.}

\item{\code{tags}}{([dict]): List of tags to be passed to the processing job.}

\item{\code{network_config}}{(sagemaker.network.NetworkConfig): A NetworkConfig
object that configures network isolation, encryption of
inter-container traffic, security group IDs, and subnets.}
}
\if{html}{\out{</div>}}
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-get_args_run"></a>}}
\if{latex}{\out{\hypertarget{method-get_args_run}{}}}
\subsection{Method \code{get_args_run()}}{
Returns a RunArgs object.
             This object contains the normalized inputs, outputs and arguments
             needed when using a ``PySparkProcessor`` in a
             :class:`~sagemaker.workflow.steps.ProcessingStep`.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{PySparkProcessor$get_args_run(
  submit_app,
  submit_py_files = NULL,
  submit_jars = NULL,
  submit_files = NULL,
  inputs = NULL,
  outputs = NULL,
  arguments = NULL,
  job_name = NULL,
  configuration = NULL,
  spark_event_logs_s3_uri = NULL
)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{submit_app}}{(str): Path (local or S3) to Python file to submit to Spark
as the primary application. This is translated to the `code`
property on the returned `RunArgs` object.}

\item{\code{submit_py_files}}{(list[str]): List of paths (local or S3) to provide for
`spark-submit --py-files` option}

\item{\code{submit_jars}}{(list[str]): List of paths (local or S3) to provide for
`spark-submit --jars` option}

\item{\code{submit_files}}{(list[str]): List of paths (local or S3) to provide for
`spark-submit --files` option}

\item{\code{inputs}}{(list[:class:`~sagemaker.processing.ProcessingInput`]): Input files for
the processing job. These must be provided as
:class:`~sagemaker.processing.ProcessingInput` objects (default: None).}

\item{\code{outputs}}{(list[:class:`~sagemaker.processing.ProcessingOutput`]): Outputs for
the processing job. These can be specified as either path strings or
:class:`~sagemaker.processing.ProcessingOutput` objects (default: None).}

\item{\code{arguments}}{(list[str]): A list of string arguments to be passed to a
processing job (default: None).}

\item{\code{job_name}}{(str): Processing job name. If not specified, the processor generates
a default job name, based on the base job name and current timestamp.}

\item{\code{configuration}}{(list[dict] or dict): Configuration for Hadoop, Spark, or Hive.
List or dictionary of EMR-style classifications.
\url{https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-configure-apps.html}}

\item{\code{spark_event_logs_s3_uri}}{(str): S3 path where spark application events will
be published to.}
}
\if{html}{\out{</div>}}
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-run"></a>}}
\if{latex}{\out{\hypertarget{method-run}{}}}
\subsection{Method \code{run()}}{
Runs a processing job.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{PySparkProcessor$run(
  submit_app,
  submit_py_files = NULL,
  submit_jars = NULL,
  submit_files = NULL,
  inputs = NULL,
  outputs = NULL,
  arguments = NULL,
  wait = TRUE,
  logs = TRUE,
  job_name = NULL,
  experiment_config = NULL,
  configuration = NULL,
  spark_event_logs_s3_uri = NULL,
  kms_key = NULL
)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{submit_app}}{(str): Path (local or S3) to Python file to submit to Spark
as the primary application}

\item{\code{submit_py_files}}{(list[str]): List of paths (local or S3) to provide for
`spark-submit --py-files` option}

\item{\code{submit_jars}}{(list[str]): List of paths (local or S3) to provide for
`spark-submit --jars` option}

\item{\code{submit_files}}{(list[str]): List of paths (local or S3) to provide for
`spark-submit --files` option}

\item{\code{inputs}}{(list[:class:`~sagemaker.processing.ProcessingInput`]): Input files for
the processing job. These must be provided as
:class:`~sagemaker.processing.ProcessingInput` objects (default: None).}

\item{\code{outputs}}{(list[:class:`~sagemaker.processing.ProcessingOutput`]): Outputs for
the processing job. These can be specified as either path strings or
:class:`~sagemaker.processing.ProcessingOutput` objects (default: None).}

\item{\code{arguments}}{(list[str]): A list of string arguments to be passed to a
processing job (default: None).}

\item{\code{wait}}{(bool): Whether the call should wait until the job completes (default: True).}

\item{\code{logs}}{(bool): Whether to show the logs produced by the job.
Only meaningful when wait is True (default: True).}

\item{\code{job_name}}{(str): Processing job name. If not specified, the processor generates
a default job name, based on the base job name and current timestamp.}

\item{\code{experiment_config}}{(dict[str, str]): Experiment management configuration.
Dictionary contains three optional keys:
'ExperimentName', 'TrialName', and 'TrialComponentDisplayName'.}

\item{\code{configuration}}{(list[dict] or dict): Configuration for Hadoop, Spark, or Hive.
List or dictionary of EMR-style classifications.
https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-configure-apps.html}

\item{\code{spark_event_logs_s3_uri}}{(str): S3 path where spark application events will
be published to.}

\item{\code{kms_key}}{(str): The ARN of the KMS key that is used to encrypt the
user code file (default: None).}
}
\if{html}{\out{</div>}}
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-clone"></a>}}
\if{latex}{\out{\hypertarget{method-clone}{}}}
\subsection{Method \code{clone()}}{
The objects of this class are cloneable with this method.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{PySparkProcessor$clone(deep = FALSE)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{deep}}{Whether to make a deep clone.}
}
\if{html}{\out{</div>}}
}
}
}
