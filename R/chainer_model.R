# NOTE: This code has been modified from AWS Sagemaker Python:
# https://github.com/aws/sagemaker-python-sdk/blob/master/src/sagemaker/chainer/model.py

#' @include chainer_default.R
#' @include chainer_model.R
#' @include r_utils.R

#' @import R6
#' @import R6sagemaker.common
#' @import R6sagemaker.mlcore
#' @import lgr

#' @title A Predictor for inference against Chainer Endpoints.
#' @description This is able to serialize Python lists, dictionaries, and numpy arrays to
#'              multidimensional tensors for Chainer inference.
#' @export
ChainerPredictor = R6Class("ChainerPredictor",
  inherit = R6sagemaker.mlcore::Predictor,
  public = list(

    #' @description Initialize an ``ChainerPredictor``.
    #' @param endpoint_name (str): The name of the endpoint to perform inference
    #'              on.
    #' @param sagemaker_session (sagemaker.session.Session): Session object which
    #'              manages interactions with Amazon SageMaker APIs and any other
    #'              AWS services needed. If not specified, the estimator creates one
    #'              using the default AWS configuration chain.
    initialize = function(endpoint_name,
                          sagemaker_session=NULL){
      super$initialize(
        endpoint_name, sagemaker_session, NumpySerializer$new(), NumpyDeserializer$new()
      )
    }
  ),
  lock_objects = F
)

#' @title ChainerModel Class
#' @description An Chainer SageMaker ``Model`` that can be deployed to a SageMaker
#'              ``Endpoint``.
#' @export
ChainerModel = R6Class("ChainerModel",
  inherit = R6sagemaker.common::FrameworkModel,
  public = list(

    #' @description Initialize an ChainerModel.
    #' @param model_data (str): The S3 location of a SageMaker model data
    #'              ``.tar.gz`` file.
    #' @param role (str): An AWS IAM role (either name or full ARN). The Amazon
    #'              SageMaker training jobs and APIs that create Amazon SageMaker
    #'              endpoints use this role to access training data and model
    #'              artifacts. After the endpoint is created, the inference code
    #'              might use the IAM role, if it needs to access an AWS resource.
    #' @param entry_point (str): Path (absolute or relative) to the Python source
    #'              file which should be executed as the entry point to model
    #'              hosting. If ``source_dir`` is specified, then ``entry_point``
    #'              must point to a file located at the root of ``source_dir``.
    #' @param image_uri (str): A Docker image URI (default: None). If not specified, a
    #'              default image for Chainer will be used. If ``framework_version``
    #'              or ``py_version`` are ``None``, then ``image_uri`` is required. If
    #'              also ``None``, then a ``ValueError`` will be raised.
    #' @param framework_version (str): Chainer version you want to use for
    #'              executing your model training code. Defaults to ``None``. Required
    #'              unless ``image_uri`` is provided.
    #' @param py_version (str): Python version you want to use for executing your
    #'              model training code. Defaults to ``None``. Required unless
    #'              ``image_uri`` is provided.
    #' @param predictor_cls (callable[str, sagemaker.session.Session]): A function
    #'              to call to create a predictor with an endpoint name and
    #'              SageMaker ``Session``. If specified, ``deploy()`` returns the
    #'              result of invoking this function on the created endpoint name.
    #' @param model_server_workers (int): Optional. The number of worker processes
    #'              used by the inference server. If None, server will use one
    #'              worker per vCPU.
    #' @param ... : Keyword arguments passed to the
    #'              :class:`~sagemaker.model.FrameworkModel` initializer.
    initialize = function(model_data,
                          role,
                          entry_point,
                          image_uri=NULL,
                          framework_version=NULL,
                          py_version=NULL,
                          predictor_cls=ChainerPredictor,
                          model_server_workers=NULL,
                          ...){
      validate_version_or_image_args(framework_version, py_version, image_uri)

      self$framework_version = framework_version
      self$py_version = py_version

      super$initialize(
        model_data, image_uri, role, entry_point, predictor_cls=predictor_cls, ...
      )

      attr(self, "_framework_name") = "chainer"

      if (identical(py_version, "py2"))
        LOGGER$warn(
          python_deprecation_warning(attr(self, "_framework_name"), CHAINER_LATEST_PY2_VERSION)
        )

      self$model_server_workers = model_server_workers
    },

    #' @description Return a container definition with framework configuration set in
    #'              model environment variables.
    #' @param instance_type (str): The EC2 instance type to deploy this Model to.
    #'              For example, 'ml.p2.xlarge'.
    #' @param accelerator_type (str): The Elastic Inference accelerator type to
    #'              deploy to the instance for loading and making inferences to the
    #'              model. For example, 'ml.eia1.medium'.
    #' @return dict[str, str]: A container definition object usable with the
    #'              CreateModel API.
    prepare_container_def = function(instance_type=NULL,
                                     accelerator_type=NULL){
      deploy_image = self$image_uri
      if (is.null(deploy_image)){
        if (is.null(instance_type))
          stop(
            "Must supply either an instance type (for choosing CPU vs GPU) or an image URI."
            ,call. = F)
      }

      region_name = self$sagemaker_session$paws_region_name
      deploy_image = self$serving_image_uri(
        region_name, instance_type, accelerator_type=accelerator_type
      )

      deploy_key_prefix = model_code_key_prefix(self$key_prefix, self$name, deploy_image)
      private$.upload_code(deploy_key_prefix)
      deploy_env = self$env
      deploy_env = c(deploy_env, private$.framework_env_vars())

      if (!islistempty(self$model_server_workers))
        deploy_env[[toupper(model_parameters$MODEL_SERVER_WORKERS_PARAM_NAME)]] = as.character(self$model_server_workers)
      return(container_def(deploy_image, self$model_data, deploy_env))
    },

    #' @description Create a URI for the serving image.
    #' @param region_name (str): AWS region where the image is uploaded.
    #' @param instance_type (str): SageMaker instance type. Used to determine device type
    #'              (cpu/gpu/family-specific optimized).
    #' @param accelerator_type (str): The Elastic Inference accelerator type to
    #'              deploy to the instance for loading and making inferences to the
    #'              model. For example, 'ml.eia1.medium'.
    #' @return str: The appropriate image URI based on the given parameters.
    serving_image_uri = function(region_name,
                                 instance_type,
                                 accelerator_type=NULL){
      return(ImageUris$new()$retrieve(
        attr(self, "_framework_name"),
        region_name,
        version=self$framework_version,
        py_version=self$py_version,
        instance_type=instance_type,
        accelerator_type=accelerator_type,
        image_scope="inference")
      )
    }
  ),
  lock_objects = F
)
